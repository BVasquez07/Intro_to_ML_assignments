{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brandon Vasquez\n",
    "\n",
    "### The following work was done without collaboration from other peers within the class but with aid from the SciKit library documentation page for the various methods, variables, and functions used.\n",
    "\n",
    "## Sources used (among the documentation of the various libraries included):\n",
    "* [Plot multi-class SGD on the iris dataset from SciKit Documentation](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_iris.html)\n",
    "* [Basically what we have to do...but for the SVM classifier](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, Rando\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sepal Length  Sepal Width  Petal Length  Petal Width\n",
      "setosa           5.1          3.5           1.4          0.2\n",
      "setosa           4.9          3.0           1.4          0.2\n",
      "setosa           4.7          3.2           1.3          0.2\n",
      "setosa           4.6          3.1           1.5          0.2\n",
      "setosa           5.0          3.6           1.4          0.2\n",
      "setosa           5.4          3.9           1.7          0.4\n",
      "setosa           4.6          3.4           1.4          0.3\n",
      "setosa           5.0          3.4           1.5          0.2\n",
      "setosa           4.4          2.9           1.4          0.2\n",
      "setosa           4.9          3.1           1.5          0.1\n",
      "['setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica'] 150\n"
     ]
    }
   ],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "features = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"]\n",
    "\n",
    "match_targets = np.array([iris_data.target_names[target] for target in iris_data.target]) # match each target name to the respective target index\n",
    "iris_df = pd.DataFrame(iris_data.data, columns=features, index=match_targets)\n",
    "\n",
    "loss_functions = ['hinge','squared_hinge', 'log_loss', 'perceptron'] #loss functions to use\n",
    "\n",
    "tested_features = { #will hold the scores of the four combinations we test with the 4 loss functions we test for discussion results.\n",
    "    \"Petal Width, Sepal Width\": [], \n",
    "    \"Petal Width, Petal Length\": [],\n",
    "    \"Petal Length, Petal Width, Sepal Length\": [],\n",
    "    \"Petal Length, Petal Width, Sepal Length, Sepal Width\": []\n",
    "}\n",
    "\n",
    "print(iris_df.head(10))\n",
    "print(match_targets, len(match_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>             Sepal Length  Sepal Width\n",
      "setosa               5.1          3.5\n",
      "setosa               5.4          3.7\n",
      "setosa               5.4          3.4\n",
      "setosa               4.8          3.1\n",
      "setosa               5.0          3.5\n",
      "versicolor           7.0          3.2\n",
      "versicolor           5.0          2.0\n",
      "versicolor           5.9          3.2\n",
      "versicolor           5.5          2.4\n",
      "versicolor           5.5          2.6\n",
      "virginica            6.3          3.3\n",
      "virginica            6.5          3.2\n",
      "virginica            6.9          3.2\n",
      "virginica            7.4          2.8\n",
      "virginica            6.7          3.1         Sepal Length  Sepal Width\n",
      "setosa           5.1          3.5\n",
      "setosa           4.9          3.0\n",
      "setosa           4.7          3.2\n",
      "setosa           4.6          3.1\n",
      "setosa           5.0          3.6\n"
     ]
    }
   ],
   "source": [
    "sep_feats = iris_df[['Sepal Length', 'Sepal Width']]\n",
    "\n",
    "print(type(sep_feats), sep_feats[::10], sep_feats.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "## Large Delta of two feature classifications when using Perceptron loss function\n",
    "* Choosing two distinct sets of features it was observed that the loss functions effect on the classifying the data drastically changed as observed in the 'Scores across the various # of features' it can be seen that when we compare the accurracy of the perceptron loss function on the two different sets of features this was where the greatest disparity was seen. When now reflecting on the plot featuring the decision regions of the respective classes we see that for the set consisting of the features 'Petal Width, Sepal Width' there was a gross misclassification for the Versicolor class where the plotted training points extend into the decision regions--not to be confused with the hyperplane--of both setosa and virginica. Now comparing this classification plot to that of the set with 'Petal Width, Petal Length' it can be observed that the higher score of the classification is due to the more accurate split of the decision regions where the training data--while still not completely linearly seperable. This delta between the two further shows the effect that linearly inseperable data has on the percetron loss function and thus the classifier as a whole.\n",
    "\n",
    "## Accurracy Across the chosen combination of features...\n",
    "* When looking across all of the features in the plot 'Scores across the various combination of features' tt is also noted that using the perceptron loss function on the three-feature combination as well as the two distinct two-feature combinations resulted in the lowest accuracy score, however, the four-feature combination exhibited the second highest accuracy score with the perceptron loss function. The highest accuracy score recorded using the four loss functions for the ‘Sepal Width, Petal Width’ two-feature and four-feature combination was using the hinge loss function meanwhile for the ‘Petal Width, Petal Length’ and the three-feature loss function the highest accuracy score was recorded with the squared hinge loss function. \n",
    "\n",
    "* Inclusively the 'Petal Width, Sepal Width' two-feature combination exhibited similar deltas in accurracy and even slightly converged on the loss function that resulted in the highest accuracy score for both--squared hinge--this could have possibly been due to the similarity in the distribution of the training data, when observing the 'loss function: squared_hinge with score 0.933' from the 3D plot of the three feature classification combination and the 'loss function: squared_hinge with score 0.933' in 2D featuring the 'Petal Width, Sepal Width' two-feature combination it can be observed that the distribution of the three species is quite similar with the setosa class being linearly seperable from the other two class meanwhile versicolor and virginica are not linearly seperable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
